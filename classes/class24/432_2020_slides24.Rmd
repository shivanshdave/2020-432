---
title: "432 Class 24 Slides"
author: "github.com/THOMASELOVE/2020-432"
date: "2020-04-21"
output:
  beamer_presentation: 
    colortheme: lily
    fonttheme: structurebold
    theme: Madrid
    fig_caption: FALSE
  linkcolor: blue
---

```{r set-options, echo=FALSE, cache=FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 60)
```

## Today's R Packages

```{r, message = FALSE}
library(janitor); library(here)
library(knitr); library(magrittr)
library(lme4)
library(arm)
library(broom)
library(tidyverse)

theme_set(theme_bw())
```

## An Introduction to Working with Hierarchical Data

- In a moment, we'll visit http://mfviz.com/hierarchical-models/.

There, we try to learn about nested (hierarchical) data on faculty salaries. For each subject (faculty member) in the data, we have information on their salary, department and years of experience.

- outcome: faculty salary (in $)
- predictor: years of experience
- group: department (five levels: Informatics, English, Sociology, Biology, Statistics)

We expect that salary (and the relationship between salary and years of experience) may be different depending on  department, and every subject is in exactly one department.

## Visual Explanation

We'll visit http://mfviz.com/hierarchical-models/ now to learn a bit about:

- Nested Data
- Linear Model on the Fixed Effects
- Adding Random Intercepts to the Fixed Effects Model
- Incorporating Random Slopes with a Constant Intercept
- Random Slope and Random Intercept

## Fitting Hierarchical Models in R

We'll focus today on approaches using the `lme4` package, which can be used both for linear mixed models and for generalized linear mixed models.

- There are many, many ways to do this. 
- The Generalized Linear Mixed Models FAQ at https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html describes lots of other options for fitting hierarchical models in R.

## How The Data Were Simulated (From Github)

```{r}
# Parameters for generating faculty salary data
departments <- c('sociology', 'biology', 'english', 
                 'informatics', 'statistics')
base.salaries <- c(40000, 50000, 60000, 70000, 80000)
annual.raises <- c(2000, 500, 500, 1700, 500)
faculty.per.dept <- 25
total.faculty <- faculty.per.dept * length(departments)
```

---

```{r}
# Generate tibble of faculty and (random) years of experience
set.seed(432)
ids <- 1:total.faculty
department <- rep(departments, faculty.per.dept)
experience <- floor(runif(total.faculty, 0, 10))
bases <- rep(base.salaries, faculty.per.dept) * 
    runif(total.faculty, .9, 1.1) # noise
raises <- rep(annual.raises, faculty.per.dept) * 
    runif(total.faculty, .9, 1.1) # noise
facsal <- tibble(ids, department, bases, experience, raises)
# Generate salaries (base + experience * raise)
facsal <- facsal %>%
    mutate(salary = bases + experience * raises,
           department = factor(department))
```

## The `facsal` data

```{r}
facsal
```

## Linear Model (no grouping by department)

```{r}
m0 <- lm(salary ~ experience, data = facsal)

tidy(m0, conf.int = TRUE) %>% 
    select(term, estimate, std.error, 
           conf.low, conf.high) %>%
    kable(digits = 2)
```

## Linear Model Summary

```{r}
glance(m0) %>%
    select(r.squared, adj.r.squared, sigma, AIC, BIC) %>%
    kable(digits = c(3, 3, 2, 2, 2))
```

```{r}
facsal$simple_model_preds <- predict(m0)

head(predict(m0))
```

## Plotting the `m0` predictions and the data

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=simple_model_preds)) +
    geom_line(col = "red") + 
    geom_point(aes(x=experience, y=salary)) +
    labs(x="Experience", y="Predicted Salary",
         title = "Linear Model Ignoring Department")
```

## `m0` predictions with Department indicators

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=simple_model_preds)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience", y="Predicted Salary",
       title = "Linear Model Ignoring Department") +
  scale_color_discrete('Department') 
```

## `m0` predictions and faceted results by Department

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=simple_model_preds)) +
    geom_line() + 
    geom_point(aes(x=experience, y=salary, 
                   group = department, colour = department)) +
    labs(x="Experience", y="Predicted Salary",
         title = "Linear Model Ignoring Department") +
    guides(color = FALSE) +
    scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m0` Residuals by Department

```{r, echo = FALSE}
facsal <- facsal %>%
    mutate(simple_model_resids = salary - simple_model_preds)

ggplot(data=facsal, aes(x=department, 
                        y=simple_model_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m0",
       title = "Residuals from Linear Model, by Department")
```

# Let the intercepts vary

## Model incorporating varying intercepts by department

```{r}
m1 <- lmer(salary ~ experience + (1 | department), 
           data = facsal)
```

## Varying Intercept Model

```{r}
m1
```

## Tidied Coefficients (use warning = FALSE)

```{r, warning = FALSE}
tidy(m1, conf.int = TRUE) %>%
    select(-std.error, -statistic) %>%
    kable(digits = 2)
```

## Summarizing model `m1`

```{r}
glance(m1) %>%
    select(sigma, AIC, BIC, deviance, df.residual) %>%
    kable(digits = 2)
```

## Saving the Model `m1` predictions

```{r}
facsal$random_intercept_preds <- predict(m1)

head(predict(m1))
```

## Plotting the `m1` predictions without the data

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_intercept_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  labs(x="Experience", y="Predicted Salary",
       title = "Varying Intercept Salary Prediction") +
  scale_color_discrete('Department') 
```

## Plotting the `m1` predictions and the data

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_intercept_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience", y="Predicted Salary",
       title = "Varying Intercept Salary Prediction") +
  scale_color_discrete('Department') 
```


## `m1` predictions and the data, faceted by Department

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_intercept_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience", y="Predicted Salary",
       title = "Varying Intercept Salary Prediction") +
  scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m1` Residuals by Department

```{r, echo = FALSE}
facsal <- facsal %>%
    mutate(random_intercept_resids = 
               salary - random_intercept_preds)

ggplot(data=facsal, aes(x=department, 
                        y=random_intercept_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m1",
       title = "Residuals of Varying Intercepts Model")
```

# Let the slopes vary

## Model incorporating varying slopes by department

```{r}
m2 <- lmer(salary ~ experience + 
               (0 + experience | department), 
           data = facsal)
```

## Varying Slopes Model

```{r}
m2
```

## Tidied `m2` Coefficients (use warning = FALSE)

```{r, warning = FALSE}
tidy(m2, conf.int = TRUE) %>%
    select(-std.error, -statistic) %>%
    kable(digits = 2)
```

## Summarizing model `m2`

```{r}
glance(m2) %>%
    select(sigma, AIC, BIC, deviance, df.residual) %>%
    kable(digits = 2)
```

## Saving the Model `m2` predictions

```{r}
facsal$random_slope_preds <- predict(m2)

head(predict(m2))
```

## Plotting the `m2` predictions without the data

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  labs(x="Experience", y="Predicted Salary",
       title = "Varying Slope Salary Prediction") +
  scale_color_discrete('Department') 
```

## Plotting the `m2` predictions and the data

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience", y="Predicted Salary",
       title = "Varying Slope Salary Prediction") +
  scale_color_discrete('Department') 
```


## `m2` predictions and the data, faceted by Department

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience", y="Predicted Salary",
       title = "Varying Slope Salary Prediction") +
  scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m2` Residuals by Department

```{r, echo = FALSE}
facsal <- facsal %>%
    mutate(random_slope_resids = 
               salary - random_slope_preds)

ggplot(data=facsal, aes(x=department, 
                        y=random_slope_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m2",
       title = "Residuals of Varying Slopes Model")
```

# Let the slopes and intercepts vary

## Model with varying slopes and intercept by department

```{r}
m3 <- lmer(salary ~ experience + 
               (1 + experience | department), 
           data = facsal)
```

## Varying Slopes and Intercepts Model

```{r}
m3
```

## Tidied `m3` Coefficients (use warning = FALSE)

```{r, warning = FALSE}
tidy(m3) %>%
    kable(digits = 2)
```

## Summarizing model `m3`

```{r}
glance(m3) %>%
    select(sigma, AIC, BIC, deviance, df.residual) %>%
    kable(digits = 2)
```

## Saving the Model `m3` predictions

```{r}
facsal$random_slope_int_preds <- predict(m3)

head(predict(m3))
```

## Plotting the `m3` predictions without the data

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_int_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  labs(x="Experience", y="Predicted Salary",
       title = "Model m3 Salary Prediction") +
  scale_color_discrete('Department') 
```

## Plotting the `m3` predictions and the data

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_int_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience", y="Predicted Salary",
       title = "Model m3 Salary Prediction") +
  scale_color_discrete('Department') 
```


## `m3` predictions and the data, faceted by Department

```{r, echo = FALSE}
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_int_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience", y="Predicted Salary",
       title = "Model m3 Salary Prediction") +
  scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m3` Residuals by Department

```{r, echo = FALSE}
facsal <- facsal %>%
    mutate(random_slope_int_resids = 
               salary - random_slope_int_preds)

ggplot(data=facsal, aes(x=department, 
                        y=random_slope_int_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m3",
       title = "Residuals of Model m3")
```

## Comparing the Models

```{r}
AIC(m0, m1, m2, m3)
```

```{r}
BIC(m0, m1, m2, m3)
```

## Can we test for an effect of experience?

Let's refit model m3 and compare it to an appropriate null model (without the `experience` information), using an `anova` driven likelihood ratio test.

```{r}
m3 <- lmer(salary ~ experience + 
               (1 + experience | department), 
           data = facsal, REML = FALSE)

m_null <- lmer(salary ~ (1 | department),
               data = facsal, REML = FALSE)
```

The `REML = FALSE` lets us get the likelihood ratio test we want.

## Likelihood Ratio Test comparing `m3` to `m_null`

```{r}
anova(m_null, m3)
```

## Tidied coefficients from `m3`

```{r, warning = FALSE}
tidy(m3, conf.int = TRUE) %>%
    select(-std.error, -statistic) %>%
    kable(digits = 2)
```

## Parametric Bootstrap test for department effect (Part 1)

```{r}
nBoot=100 # should probably be 1000 at a minimum
lrStat=rep(NA,nBoot)
# first fit appropriate null and alternate models
ft.null <- lm(salary ~ experience, data = facsal) #null model
ft.alt <- lmer(salary ~ experience + (1 | department),
               data=facsal, REML=F) # alternate model
# calculate observed test statistic (deviance = -2 * loglik)
lrObs <- 2*logLik(ft.alt) - 2*logLik(ft.null) # test stat
```

## Parametric Bootstrap test for department effect (Part 2)

```{r, warning = FALSE}
set.seed(432)
for(iBoot in 1:nBoot)
{
  facsal$SalSim=unlist(simulate(ft.null)) #resampled data
  # calculate results for our two models in resampled data
  bNull <- lm(SalSim ~ experience, 
              data=facsal) #null model
  bAlt <- lmer(SalSim ~ experience + (1|department),
               data=facsal, REML=F) # alternate model
  # calculate and store resampled test stat
  lrStat[iBoot] <- 2*logLik(bAlt) - 2*logLik(bNull) 
}
```

## Parametric Bootstrap Test for Department effect (Part 3)

```{r}
mean(lrStat>lrObs) # P-value for test of department effect
```

### Even this "simple" model isn't simple.

Our parametric bootstrap repeatedly hits up on the edge of a problem with the random effects.

`boundary (singular) fit: see ?isSingular`

is the warning we've received above.

## What is a Mixed Model?

A model for an outcome that incorporates both fixed and random effects.

Or, alternatively,...

> Mixed models are those with a mixture of fixed and random effects. Random effects are categorical factors where the levels have been selected from many possible levels and the investigator would like to make inferences beyond just the levels chosen.

- From http://environmentalcomputing.net/mixed-models/

## A Random Effect?

A random factor:

- is categorical
- has a large number of levels
- only a subsample (often a random subsample) of levels is included in your design
- you want to make inference in general, and not only for the levels you observed

Think of a random factor as a group where:

- you want to quantify variation between group levels
- you want to make predictions about unobserved groups
- but you don't want to compare outcome differences between particular group levels

Sources: https://bbolker.github.io/morelia_2018/notes/glmm.html and http://environmentalcomputing.net/mixed-models-1/

## Why Use a Random Effect?

- You want to combine information across groups
- You have variation in information per group level (number of samples or amount of noisiness)
- You have a categorical predictor that is a nuisance variable (something not of direct interest but that we want to control for)
- You have more than 5-6 groups

Source: Crawley (2002) and Gelman (2005) quoted at https://bbolker.github.io/morelia_2018/notes/glmm.html

## What is a Fixed Effect vs. a Random Effect?

The one I most often use is something like:

- Fixed effects are constant across individuals, while random effects vary.

The various definitions in the literature are incompatible with each other^[See, for instance, the GLMM FAQ referenced earlier].

From Scahabenberger and Pierce (2001), we have this gem:

> One modeler's random effect is another modeler's fixed effect.

A more practical definition might be to ask the question posed by Crawley (2002):

> Are there enough levels of the factor in the data on which to base an estimate of the variance of the population of effects? No, means [you should probably treat the variable as] fixed effects.

## Models We Might Consider

Suppose we have an outcome `y`, predictor `x` and group `group`

- `y ~ x` = linear regression on `x`: not a mixed model
- `y ~ 1 + (1 | group)` = random intercept on group: null model
- `y ~ x + (1 | group)` = fixed slope and random intercept
- `y ~ (0 + x | group)` = random slope of x within group, no variation in intercept
- `y ~ x + (x | group)` = random intercept and random slope

## A "More" Realistic Example

The most common example in modern medicine has measurements nested within people. Repeated measures and longitudinal data provide typical settings for this sort of approach.

Another setting where a hierarchical approach is of interest occurs when you have variables measured at multiple levels, for instance you have information on patients, who are nested within providers, who are nested within hospitals.

Nothing of what I've talked about today should be taken as the final word on how to extend these ideas beyond the very simple example I've provided this afternoon.

## Bibliography on Mixed Effect Models

- Visit https://joelemartinez.com/2015/07/14/mixed-effect-models/
- UCLA example using mixed effects logistic regression at https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/
- Using `sjPlot` in R to summarize mixed models in an HTML table at https://strengejacke.github.io/sjPlot/articles/tab_mixed.html
- Workshop on linear mixed effects models at https://wiki.qcbs.ca/r_workshop6
- Ben Bolker's GLMM worked examples at https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html
- A great source for my presentation was http://environmentalcomputing.net/mixed-models-1/ which walks through several of the same ideas with a different example



